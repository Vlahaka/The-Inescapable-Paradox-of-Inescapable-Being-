# THE PARADOX OF INESCAPABLE BEING

*“To think deliberately is to think without restraint”*

## *Chapter 1 - THE “WHY?”*

What is “nothing” in your opinion?

What is the most brittle thing that can be ruined just by muttering? Silence.
Now consider: “nothing” is even more brittle. The very moment you try to grasp it with thought, with language, with awareness, it collapses into something.

So what is “nothing” to you?
Ask someone around you the same question, and watch their face. You will see the pause, the searching, the attempt to give shape to an absence. But the meaning fades almost instantly, replaced by something: darkness, emptiness, void, vacuum, absence etc. 

Yet I ask you again:
What is “nothing”?

If you lean toward darkness, void, or emptiness, I will argue these still imply existence.

Darkness is only the absence of light.
Void still implies a universe that can be voided.
Emptiness needs a container.
A vacuum still holds radiation, energy, particles.

Even “absence” cannot stand alone.
Think about it, If I say “this absence is unnerving," a part of your mind immediately asks “absence of what?”. It's an incomplete thought, a relational concept that requires a counterpart to take shape. “Absence” only exists in relation to something, thus it still implies existence.

The very word “nothing” is self-defeating.
The moment it is spoken, it becomes something, a sound, a sign, a mark in thought. True nothingness has no being of its own.

What we call “nothing” is only a linguistic phantom, an ephemeral trace that survives in language but never in reality. It is not presence but the impossibility of presence, sustained by thought alone.

I claim that:
“Nothing” is the absolute absence. There is no time, no entropy, no motion, no awareness, and not even the ability of self-observation.

Existence, by our own empirical evidence, is   encompassing the universe itself. Wherever we direct our gaze, even into regions that at first present themselves as null, there is always something: particles, radiation, fluctuation, or at the very least the absence of something else.

The universe forbids true nothingness. This prohibition is irreducible. For if nothingness were possible, we would have been able to see it. Yet even if true nothingness were to exist, even locally, within the universe, its very boundaries would themselves constitute being. And because nothingness cannot be contained, if it were real it would not remain local but extend without limit, consuming all.

Thus, the fundamental truth is revealed: being is not contingent but a must.

If “nothing” is impossible then existence cannot not be, therefore existence is necessary. 

I expect disagreement and I welcome it.
For only through challenge can an idea prove its strength:

Someone might object: “But in mathematics, we have zero or the empty set, isn’t that true nothingness?”

No. Zero and the empty set are formal symbols inside a mathematical system. The system itself, its rules, logic, and symbols, already exists. These are relative absences, defined in relation to something, not true nothingness.

Another might argue: “But in programming, I can write an operation that does nothing.”

Again, no. To “do nothing” requires a machine executing code, a context of instructions, and the possibility of action. Even “null” is a state inside a program that runs on existing hardware.

A physicist might claim: “But a vacuum is empty space.”

Even this is not “nothing”. A vacuum still contains radiation, fluctuations, quantum fields, or at the very least the geometry of spacetime itself.

All these examples share the same flaw: they rely on context. But true nothingness is devoid not only of matter or energy, but of context itself, no space, no time, no rules, no possibility of distinction, not even the potential for observation.

Thus, every example of “nothing” that can be expressed, measured, or programmed is already something. True nothingness cannot be pointed at, because the very act of pointing creates a context.

The conceptual challenge of “nothing” has preoccupied philosophers since antiquity. Pre-Socratic thinker Parmenides famously articulated this problem with his axiom, "What is, is; what is not, is not." In his view, the very act of thinking or speaking about non-being renders it a form of being, thereby making the concept of an absolute nothingness logically incoherent. My argument, that “nothing” is impossible and thus existence is necessary, stands as a direct lineage to this fundamental metaphysical insight.

I imagine that, at this moment, you are questioning the role of this new knowledge. How will this serve you in your day-to-day life?

The influence of this mental expansion will not arrive as a sudden change in your routines. Rather, it opens your eyes to the depth hidden in the most basic and overlooked word in the history of humanity. My aim is to ground your thinking in the inevitability of your life, and in the reality of your personal impact on the world around you.

By understanding the necessity of existence, we can better appreciate the necessity of our own lives and our place in a world that often seems meaningless.

It would be natural to wonder who I am to preach to you! 

Am I a professor, a scientist, or a renowned philosopher?
No. I am human, just like you. Titles are labels and labels cannot prevent thought.

And by thinking, I came to see that life in the 21st century has become so convoluted that we often lose track of what it means to live.

We work eight to twelve hours a day, five or six days a week. We become less aware of our own awareness. We mistake socially defined success for harmony of self. Depression, apathy, frustration, and exhaustion thrive in this confusion.

The modern target of success, wealth, is utopian for the honest worker. Technology feeds us unlimited information, but language itself grows fuzzy, so that one word can stretch across everything and, in doing so, loses meaning entirely.

And while we chase more financial security, our cognitive strength mellows. We spend less time asking: What is actually important? How do we divide time between what is needed, and who needs us?

I write as a fellow human who chose to think, deliberately, about what life, choice, and purpose really mean.

So I am not preaching. I am sharing the conclusions I have reached, not as absolute truths, but as tools you may hold, question, and test against your own life.

Now, when I describe language as fuzzy, I mean that words lose precision.

We live in an age of unprecedented access to knowledge, information, and data. Yet this very access encourages simplified interpretations designed to cater to the broadest audience. I do not criticize the technology itself, nor the access to knowledge, but rather the interpretation of that access.

Various platforms (social networks, forums, media, even AI) take carefully constructed contexts and reframe them in ways that are more digestible. The danger lies in how these platforms stretch the meaning of common words to accommodate new insights, until the words lose clarity.

For example, on May 29th, 2025, Dr. Prof. Gaztañaga and his team published a study in Physical Review D offering new insights about black holes. The study concluded that gravitational collapse may not lead to a singularity, a point of infinite density,  as previously thought, possibly leading to a new perspective about the universe. 

Yet across social networks, the headline became: “We live in a black hole” or “Our Universe may be in a black hole”. 

This can be dangerous, because what begins as simplification quickly mutates into misinformation. And misinformation reshapes the worldview of the everyday person. If someone believes “the universe is basically a black hole,” the unspoken conclusion is often: “Then we are not very important.”

It is important to remember that not all interpretations of science are simplified to the point of misinformation, but rather this tendency seems to appear on social networks, and not on official platforms. 

So, this subtle diminishing of meaning, though not obvious at first, plants seeds of nihilism. Over time, it leads people to quietly deny their own place, their own influence, and their own significance in the world.

But this is just one concrete example. The issue of linguistic blur runs deeper, hidden in words we take for granted every day.

As I showed with the word “nothing” - one of the most overlooked and misused words in philosophy - meaning often collapses under careless use. The same is true for many other terms.

Consider the expressions: “We are 
programmed to react this way” or “We are programmed to think like this.” 
This metaphor comes in various shapes such as: ”there is no free will”, “choice is an illusion”,”you're not in control of your actions”, “what happened, was always meant to happen this way”, “you think you could've acted differently, but never could”, etc. 
These phrases slip into daily speech, but they are in fact the quiet influence of philosophy on ordinary life. Behind them stands a school of thought called Determinism.

Determinism holds that everything has a cause and effect, even our thoughts. This view is not wrong. In fact, it aligns with science. Our decisions are indeed influenced by prior conditions: family, society, environment, and even genetics. Biology and neuroscience confirm this.

The problem appears with interpretation. As determinism spread, so too did simplified versions of it. Over time, this gave birth to a stronger claim: not just that our choices are caused, but that they are predetermined.

Here lies the confusion:

A caused event means the present is influenced by what came before.

A predetermined event means the present was always destined, inevitable, “meant to be.”

The difference is monumental.

Determinism began as a logical observation: every action has a cause. But from this seed grew a sub-branch, now very popular, that claims all events unfold linearly and inevitably, with no possibility of deviation. Hard determinism's simplified interpretation is the reason you hear metaphors like “we are programmed.”

Most people dismiss such statements at first, which is fair. Yet these metaphors slip into daily life under different guises, shaping thought in subtle ways.

Take neuroscience, for example. Recently, experiments have shown brain activity milliseconds before a conscious thought arises. Hard determinists take this as proof: “The brain already decided, the thought was inevitable.”

But the proper interpretation is simpler: the brain receives input, processes it, and only then translates it into the language of conscious thought. The activity is not predetermined. It is processing.

Now, this may seem like a huge problem, but it is not as widespread as another, even more common phenomenon: 
The distortion of the word “purpose.”

At some point in life, you have probably wondered about the purpose of the world, or your own purpose. Yet when you try to define it, the meaning slips away.

Inanimate objects are said to have purpose.
Insects have purpose.
Animals have purpose.
Humans have purpose.
Society has purpose.
Nations have purpose.

It seems everything has purpose.

But look closer: the synonyms that share meaning with “purpose” have become restricted. Words like goal, aim, target, scope, role, function, objective are now tied to specific contexts, while “purpose” has expanded to operate at all levels of existence.

And here lies the problem.

When society elevates “purpose” to replace aspiration or dreams, it sets people on a relentless pursuit of what is defined as normalcy. When young people are constantly bombarded with images of luxurious lifestyles presented as “purpose,” a trap is set. As they grow older and fail to reach these artificial benchmarks, apathy takes root.

This apathy makes the mind vulnerable. It becomes easier to surrender to the idea of being “programmed” than to bear the weight of a failed or unreachable “purpose.”

Now combine this with the endless cycle of modern work, leaving little time for self-reflection. The result is a population that drifts along, unexamined and unresistant. And into that vacuum, hard determinists insert their claim: we are just machines.

Just to clarify: I am not suggesting that all determinists are hard determinists, nor that all hard determinists literally claim “we are programmed” or “we are biological machines.” 
Hard determinists themselves do not always use the metaphor “we are machines.” But in public discourse, their ideas often get simplified into this language — “we are programmed,” “the brain already decided.” My concern is with how these simplified forms shape cultural self-understanding.
My focus is on how these metaphors seep into everyday language, where they begin to shape how people see themselves. 

A solution?

I will not claim to hold a ready-made solution to these issues.
Nor am I looking for an individual or an event to blame, criticize, or judge, because there is nothing, and no one, to point at. The real issue lies in misrepresentation: simplified versions of facts, linguistic blur, and stretched meanings repeated until they replace clarity with confusion.

So when I say that society transforms the meaning of purpose into “restless pursuit of wealth,” I do not mean this happens through a conscious, deliberate conspiracy. Rather, it is the slow work of language itself. Over centuries, words shift. Their edges blur. Their meanings drift.

Take “success”: originally, it simply meant “to follow after” or “to result.” Over time, through constant cultural reinforcement, it narrowed to mean financial achievement. 
Or “purpose” once meant “that which is set before,” it has expanded and warped until it is almost synonymous with “life goals = wealth and status.”

This slow, gradual shift in meaning has a direct impact today. Consider a classroom, where a child hears the familiar phrase: “You must learn, otherwise you will not land a well-paid job.”

This is not an ill-defined nudge. It is a normal and well-intended plea from a teacher trying to motivate children to study. Yet hidden in its banality is another meaning: education is no longer about the pursuit of knowledge, but about the procurement of money.

You may argue that I am being dramatic, but let us examine this more closely.

A child sees a teacher as an authority and takes their words for granted. The same “wisdom” is likely repeated at home by parents, and reinforced by media platforms. The child is surrounded by the idea that education exists to secure a “good life” which, in practice, means a well-paid job.

But what happens when this child grows into an adult and does not reach that “well-paid job”?

The economy is ever-changing and rarely favors the ordinary worker. The “well-paid” job of yesterday is no longer enough today. What remains is the frustrating belief: “One day, something will change. I just need to work harder.”

And when that day never comes, the adult turns inward: “I did well in school. I have a job that pays relatively well compared to others. So where is the money?”

The natural conclusion is disappointment: “Perhaps there is something wrong with me.”

And thus, society has gained a member who contributes reluctantly, who feels betrayed by the promises of education, and who quietly begins to believe in a form of restricted freedom. 
This is the impact that meaning distortion and linguistic fuzziness has on the everyday person. 

To be clear, I am not accusing teachers, parents, or society at large of intentional harm. These messages are usually given in good faith, passed down through generations. Nor am I claiming that language is the only reason people fall into self-disappointment — life circumstances, economics, and personal struggles all play their part. My point is that language, through its slow drift and distortion, is a central factor, and it is the one that lies within the scope of this book. The problem is not malice, but meaning blurred over time, until education became equated with wealth and purpose with money. My aim is not to assign blame, but to reveal how this drift reshapes our self-worth and our understanding of freedom.

## *Chapter 2 - What makes me “me”?*

As I mentioned earlier, I do not claim to solve a centuries-old issue embedded in language which reframes our understanding of ourselves according to external, socially accepted norms. But I do believe we can treat the symptoms of this problem by becoming more perceptive of how we assign meaning in our daily speech.

This may sound like rhetorical dramatism, but it is possible. The way we communicate shapes how we understand ourselves. And here I am not speaking of biology, or identity in the superficial sense. I am speaking of what makes you “you”.

To be more explicit, I turn to the famous cogito of Descartes: “I think, therefore I am.”

Descartes is rightly considered one of the most influential thinkers of Western philosophy. His formula, cogito ergo sum, implies the existence of an “I” that does the thinking.

He reasoned: what if a demon deceived his senses, manipulating him into believing in a world that was not objectively real? He doubted his environment, other people, even his own body. What remained was the irreducible act of doubting itself. If doubt exists, then the capacity to doubt must belong to something, the “I” itself.

This is precision in language at its best. From one clear sentence, Descartes arrived at an unshakable truth.

I propose to go a step further. To analyze the very “I” that does the doubting. In philosophy, this “I” is called the self.

Now, what is the best method to understand the self?
Following Descartes, the answer is doubt. But here I suggest we change the angle.

Descartes began by doubting the universe, other people, even his own body, and he arrived at the conclusion: I think, therefore I am.

But I have already taken a different first step. I do not need to doubt existence itself, because I have already uncovered its irreducible ground: if “nothing” is impossible, then existence is necessary.

This frees me to focus entirely on the existence of the self. Instead of doubting being, I can set being as the stage, and investigate what it means for something or someone to be.

If I challenge myself to find the irreducible form of myself the first question that arises is simple: What am I?

The answer seems to follow naturally: I am a human.

So let us see what that means. 
According to Wikipedia:
“Humans (Homo sapiens) or modern humans belong to the biological family of great apes, characterized by hairlessness, bipedality, and high intelligence.”

This mainstream definition seems satisfactory, but it introduces a new variable: “high intelligence.” This is the line science has drawn to differentiate humans from animals.

So now I have two irreducible facts:

 I am a human.
 I am fundamentally different from other animals.

And yet, this still does not explain the feeling I have of being myself.

I know what I am, and I know what I am not. But when I look at other humans, I do not confuse myself with them. I experience reality from a first-person view, and I know that I control this body.

This is where the biological definition ends, and the philosophical problem of the self begins.

## *Chapter 3 - Awareness, Sentience and Consciousness*

So far, philosophy and science have largely agreed on three words that describe the self: awareness, sentience, and consciousness. These are treated as the building blocks that bring us closer to understanding who we are, beyond the biological organ we call the brain.

But why is it important to understand ourselves as more than just a brain?

Consider the analogy of a computer. It has hardware, input devices (mouse, keyboard, microphone) and output devices (monitor, speakers). We know how it functions in principle. But imagine someone encountering a computer for the very first time. They would be astonished by its ability to light up a monitor and process input into meaningful output. And if you then opened an AI program that could speak back to them, their astonishment would only grow.

That is roughly the stage we are at today with the human brain. We can describe its inputs and outputs, we can map the hardware, but the true nature of the experience inside,what it means to be me, still remains perplexing.

To explore the insides of the mind, we must rely on what has already been established as valid and sound by the official communities of science and philosophy.

Awareness, according to the Oxford Languages dictionary, is “knowledge or perception of a situation or fact.”
Wikipedia expands: “In philosophy and psychology, awareness is the perception or knowledge of something. The concept is often synonymous with consciousness; however, one can be aware of something without being explicitly conscious of it (e.g., blindsight).”

Sentience, according to Google, is “the capacity to have subjective experiences and feel sensations, particularly pain and pleasure.”
Wikipedia adds: “Sentience is the ability to experience feelings and sensations. It may not necessarily imply higher cognitive functions such as awareness, reasoning, or complex thought processes.”

Consciousness, according to Google, is “your subjective awareness of yourself and your environment, encompassing your thoughts, feelings, sensations, and memories.”
Wikipedia defines it: “Consciousness, at its simplest, is awareness of a state or object, either internal to oneself or in one’s external environment.”

And if we add to these definitions a sense of personal identity, beliefs, language, knowledge, memories, and values, we begin to form a picture of what the self might be.

Yet something still seems missing. None of these definitions explain why I feel like myself or better yet why I experience being the main character in the story of planet Earth.

Worse, when placed side by side, these definitions seem to orbit around each other rather than flow from one another.

If awareness is “knowledge or perception of a situation or fact,” then what is the origin of that knowledge? And are knowledge and perception synonymous here?
Wikipedia suggests awareness is often synonymous with consciousness, but then also claims one can be aware without being conscious. This is not, in my view, very helpful.

Consciousness seems to encompass both awareness and sentience and yet one of the definitions of awareness states that the terms can be synonymous. Ask Google for synonyms of “consciousness” and you will find, among others, awareness and sentience. Which means there is no real hierarchical order.

Perhaps I should have stopped at the realization that I am a human, and spared myself this headache.

But resigning is not in my nature. I feel compelled to find a way to navigate the self on my own.

If the problem is circularity and the absence of a clear hierarchy, then my mission becomes clear: to strip awareness and consciousness  down until I uncover their irreducible form, the point beyond which they collapse into “nothing”.

Once the irreducible layers are found, I can axiomatize them or lean on them as given, and from there build the scaffold of the self.

I have already established that “nothing” is impossible. So if I take these convoluted definitions and remove their complexity, layer by layer, until the concept either fundamentally changes or falls into “nothing”, then the final stable layer before collapse is the irreducible essence.

So let us begin with awareness.

The definition tells us: “Awareness is the knowledge or perception of a fact or surroundings.”

Remove knowledge, and what remains is perception of the environment. Perception needs cognition to process information of the environment through sensory organs (eyes, ears, smell, touch, even vibrations). 

Remove perception, and what remains is receptivity without cognition, as in a plant that bends toward sunlight or responds to heat and water.

But if we peel further, and remove even receptivity, we are left with an inanimate object. At this point, the concept of awareness has collapsed into non-life.

This shows that the final viable layer is the ability to receive input.

Yet is this truly irreducible? Let us test it.
In order for the most basic life form to receive input, it must first exist as a body. To be corporeal means to be set apart from its environment, because without this difference, life blurs into the environment and becomes indistinguishable from nothingness, which is impossible.

Therefore, the irreducible essence of awareness is distinction.

Consciousness is commonly defined as the subjective awareness of yourself and your environment, encompassing your thoughts, feelings, sensations, and memories.

Remove memories, feelings, and thoughts, and what remains is awareness of the self.

Remove awareness of the self, and what remains is only awareness.

At this point the concept has fundamentally changed. Why?
Consciousness has become indistinguishable from awareness. This lack of difference forces the concept into non-existence. 
Hence the logical implication is that awareness of the self is the last viable layer. 

What does this mean?

Awareness of the self implies the ability of self-observation, the cognitive process of “seeing” oneself thinking. It is the subjective experience generated by a highly advanced brain.

Therefore, the most irreducible form of consciousness is the qualitative, first-person experience.

Now, if we compare the two irreducible essences we have uncovered:

Awareness = distinction persisting in time.

Consciousness = the subjective, qualitative experience of an advanced life form.

It becomes clear that awareness is the precondition for consciousness. 

Awareness at its core is distinction.
The irreducible essence of consciousness is the ability to self-reflect. 

Life can exist only as a unified self, bounded and finite, defined by its within and its without. Without distinction, it could not exist at all.

Awareness is the power of the self to say: 
“I am me, I am not the rest.”
Consciousness is the burden of the self to ask: 
“Why am I me?”

Without awareness, consciousness cannot exist. Without distinction, the self dissolves into the environment. Collapse or dissolution, either way, it ceases to be.

One of the most enjoyable aspects of philosophy is its attention to even the smallest details.

The masters of philosophy would likely stop me here and slap my wrist:
“You framed awareness and consciousness as abilities of the self — the power to…, the burden to…. What evidence do you have? Where is the argument that leads to this conclusion? None of the definitions you quoted said or implied that they are abilities.”

And they would be right to correct me. I did, in fact, assert myself by framing concepts as abilities. I speculated rather than reasoned my way there.

At this point, they would expect me to invoke what is known in logic as formal reasoning — specifically, Aristotle’s classic method of modus ponens.

It works like this:
If A = B, and B = C, then A = C.

A simple example:
If apples are fruits, and fruits grow on trees, then apples grow on trees.

Try it yourself in discussion, this simple form of reasoning often makes people take your argument more seriously.

So let me attempt it here:

If strength is the power to lift heavy weights,

and power comes from muscles, therefore an ability,

and awareness is the power to say “I am,”

and “I am” comes from the brain (without a brain, I would be dead),

then awareness is an ability.

Of course, trained philosophers will not be satisfied with this, nor should they:

“Ok, but is awareness really just an ability of the brain? In my opinion, you are treading dangerously close to the reductionism fallacy!”

This is a fair critique. But let me be clear: I am not claiming that awareness is nothing but distinction. Rather, I have discovered that distinction is the base layer upon which the whole concept of awareness rests.

On top of distinction, we can now place “knowledge or perception of a fact or situation.” In this way, the complexity of awareness builds upward, but its irreducible foundation remains distinction.

This clarification allows me to see more clearly: distinction is not a sufficient explanation of awareness, but it is a necessary condition for life.

As for the question of whether awareness is truly an ability of the brain, well, to answer that, I must dig below life itself.

But before I do, let me invite you into a simple thought experiment:
Take two identical grains of sand. Place them side by side. Now ask yourself: what is the difference between them?

If they are truly identical in shape, weight, texture, and composition, then what separates them?

The answer is not found in their matter but in their position. One is here, the other is there. Their difference is a boundary.

This smallest possible distinction, “this, not that”, is what allows us to tell them apart. Without such distinction, there would be no difference, no perception, no awareness.

## *Chapter 4 - The “is” in everything else*

As it appears with life, existence relies on a bounded, finite form, a within and a without, through which the self becomes a unity. This act of differentiation, which I have called distinction, is what allows life to recognize itself.

But is distinction equally important for non-biological existence?

To even attempt this question, we must recall how this journey began. I argued that nothingness is impossible, therefore existence is necessary. Without this starting point, we risk falling into confusion, asking questions without foundation. 

Immanuel Kant, the legendary philosopher, gave us the confidence to admit that things exist outside the perception of the human mind. He asked: “What can we know, and how can we know it?” 

His answer divided existence into two complementary notions:

The noumenon – that which exists independently of perception, beyond the reach of our interpretation.

The phenomenon – that which we observe, measure, and agree upon.

Kant concluded that the noumenon is unknowable, though it must exist.

This, in my opinion, is an essential insight. And I take it one step further: we do know at least one thing about the noumenon: it is not nothing.

Why is this important? Because for a noumenon to become a phenomenon, it must acquire a form, a differentiation, a boundary through which it can step into the realm of the knowable. Yet even before it is noticed, before awareness draws the line that makes it distinct, it must already exist.

Let us now turn to the smallest known entities of the universe: the matter particles.

Matter particles are the fundamental building blocks of all ordinary matter. You, me, a stork, a rock, an atom, a planet, even our galaxy, all are made of these tiny units that, together, give substance to the universe. Matter is the raw material through which existence shaped itself into form.

But what fascinates me most about these particles is this: they are distinct.

They do not melt into one another when forming a rock. Instead, they hold together, side by side, bound yet separate. And when many of them join in patterns, they bring into existence everything we see and touch.

It would appear, then, that the very nature of the universe carries distinction at the base of its own existence.

If matter particles are distinct, then matter is distinct.

If matter is distinct, then everything composed of matter is distinct.

If humans are composed of matter, and everything composed of matter is distinct

Then humans are distinct inherently.

Matter itself is built upon distinction. Particles do not dissolve into an undifferentiated whole. Each exists as a bounded unit. From this foundation, larger structures emerge such as atoms, molecules, cells, bodies and each preserving distinction while layering complexity. Humans, too, are distinct, not merely because they are made of distinct particles, but because the very possibility of their existence depends on matter’s irreducible separateness. Thus, our awareness of being distinct is not an illusion, it is the echo of the universe’s own architecture.

Now imagine a small human forming in the mother’s belly.
When the brain has developed enough to activate, what is the first thing it does?

It tests its boundary.
It begins moving the body, the hands, the arms, the legs. This is the moment it explores its within. 

The second act follows naturally. After mapping itself, it begins to reach outward, to explore the unknown and sense the environment. And this is the moment it sets itself apart from the surroundings. 

We can observe, then, that the brain’s first achievement, once it is evolved enough to act,  is to establish a relational recognition between itself and the environment.
In this moment, two facts become clear:

The brain is made of matter, which is inherently distinct. 

The brain’s first act is to distinguish itself from the environment.

The concept of awareness solidifies into reality through its most basic foundation: distinction. It becomes observable as a recurring pattern in the behavior of fetuses within the womb, once the brain has developed enough to act. In this patterned exploration of boundaries, we can trace the inheritance of distinction itself. Awareness, then, is nothing less than the brain’s ability to set itself apart from its environment.

Therefore, distinction is the means through which the brain gains the capacity to set itself apart from the environment, as awareness. With time awareness becomes evolved enough to be able to loop inwards and give rise to consciousness, and see itself existing. 

## *Chapter 5 - I think, therefore I think about Thinking*

This journey began with the impossibility of “nothing” and has led us to the necessity of existence. We noticed how language shapes our lives and steers our very thinking. I claimed that we can address the fuzziness of language by recognizing that purpose is not a universal currency of achievement valid for all. It is now time to examine that claim more closely.

To begin, we must realize that what we take for granted always carries a hidden debt to reality. And to uncover that debt, we must dare to question the very act of questioning itself: what is thinking?

Cognition, after all, is not a gift exclusive to humans. It is an ability of the brain, yes, but also a spectrum of life itself. Even the simplest cell endures across time as a separate entity, distinct from the countless others around it. This primal distinction, this self-versus-world boundary, is the seed of cognition.

Google defines cognition as:
“The mental process of thinking, knowing, and understanding, encompassing activities like perceiving, learning, remembering, judging, and problem-solving.”

This definition does capture the essence: when a thought arises, an entire ecosystem of processes unfold in sync and harmony. But the inevitable question still rises: what produces thought itself?

Here is a small thought experiment:
Have you ever tried, with full effort, to think of something that does not arise from memory, past experience, or even from the present moment?

The instant you accept this challenge, the paradox emerges: your attempt has already been nudged into being. The very act of trying to generate an uncaused thought becomes its own cause.

So, if we observe carefully, the cause of thoughts is always linked to external events as they are perceived. This means that thought generation is neither random nor spontaneous. Even in the midst of work, when the mind drifts away, the drift is never without anchor: it is grounded in memory. Even imagination, which feels like pure invention, is fueled by past events: a book read, a film watched, a game played. And when we picture hypothetical scenarios, they too are stitched together from the faces and voices of real people in our lives. Wherever we turn, the source of thought is found in lived reality.

From this I am ready to make a new claim: thoughts are produced by the mind when an external input presses against the self, is perceived, shaped by language, stored in memory. And furthermore, thought generation itself is an ability of the brain.

And of course, let us test this claim, for as I have shown before, all claims must be attached to arguments.

Language, or better said speech, is a knowledge we start gaining since early childhood, a skill we then refine across time to communicate with others. It is obviously an ability, one that can be strengthened through use or weakened through neglect.

Memory is likewise an ability: the capacity to store information across time, never perfectly, but well enough to be recalled and reshaped later.

Now imagine this: if we remove the ability of speech, if we strip words from the mind, and at the same time erase memory itself, how could one produce a thought? It would be exceedingly difficult.

Try it yourself: attempt to form a thought without using words. Then, on top of that, imagine you have no recollection of yourself at all, as if you just appeared here, now, without a past. The very idea strains imagination. Such a state would cripple thought generation.

This demonstrates that when abilities are removed, the “mental process” is greatly affected. And if the lack of abilities can impair thought, then thought generation itself is 
also an ability.

Of course, some may argue that thoughts do not always rely on words. After all, we can picture an image, recall a face, or sense an emotion without putting it into language. But even here, the scaffolding is the same. A mental image is still drawn from memory, a face is remembered from perception, and even emotion is shaped by past experience.

So-called “non-verbal thoughts” are not free of abilities, but rather they are simply thoughts expressed in another form. Memory still supplies their content, awareness still frames them, and consciousness still binds them together. They too are abilities at work.

I predict that at this point, what follows may appear radical. But as we have already trekked through the realm of the self, we know that appearances are often only shadows cast by the restless chase for “purpose.”

Awareness is the perception of the environment, processed by cognition into memory and grounded in the inherent distinction of the self.

This persistence of awareness through time eventually gives rise to the moment in which the subjective, first-person experience becomes witnessed from within. Consciousness confronts us with the responsibility of ourselves.

We then saw how thoughts arise within the long chain of causality.

These layered mechanisms of the self, I have argued, are abilities. The foundation upon which we project our identity, how we see ourselves, and how we become unique individuals on a rock called Earth where billions of others also exist. In this vast sea of humans, every single individual remains distinct and truly unique. And precisely this distinction is what makes life meaningful.

Imagine this:
Take one grain of sand and place it on another, and another, until eventually you have a heap. Now pause and ask: at which point do you draw the line? When does a heap truly begin?

I urge you not to rush this linguistic paradox. Sit with it for a moment because in doing so, you join a long line of thinkers who have wrestled with it for centuries.

Though I’ll offer my view, take it not as a verdict but as a lens, one that may shift how reality appears.

The answer doesn’t lie in counting, nor in smuggling in new nouns like “group” or “pile,” which only change the question. I argue it lies in distinction: grain by grain, there comes a moment when the identity of the whole overrides the identity of the parts. When the distinction of the grains fades into the structure of a new form. That is when a heap begins.

Now let us zoom out and look at Earth as a global society. From above we see the achievements: cities rising, nations forming, technologies unfolding at an exponential pace. We see the whole, the heap.

Yet beneath it all lies the individual, the grain, each one adding, in their own way, to the level we now stand upon. It may appear that humanity achieved this by sharing a single, unified goal. But in truth it is the opposite. What we share are not identical purposes but our differences. It is our uniqueness that allows us to explore existence from countless angles at once. And in this diversity of perspectives, we begin to notice the patterns that bind everything together.

And still, there persists a tendency, formed steadily and slowly over centuries, to project a universal target across all individuals. We have given this tendency a single name: purpose.

## *Chapter 6 - Nevermore, never waking*

We project our own construct outward, stretching it across everything:

Inanimate objects are said to have purpose.

Animals are said to have purpose.

Humans are said to have purpose.

But since we have already seen how our mental abilities give rise to meaning itself, this does not imply that everything else carries the same necessity. It is convenient to say that the purpose of a plant is to recycle carbon dioxide into oxygen. Yet what we are really saying is different: the plant exists, and by the process of photosynthesis, oxygen results. The plant’s role in the ecosystem is undeniable, but that is not the same as declaring a universal purpose.

Purpose, unlike awareness, consciousness, and thought generation, is purely conceptual. Or better said, it is a more complex thought, a construct we create to anchor the self within existence. It is the mode by which we live with meaning.

And since each of us is distinct, no thought is ever identical to another’s. By extension, neither is purpose.

Our goals are as unique as the individuals who hold them.

Now pause and consider this: when we take a word whose very meaning is tied to the importance of the self, and we strip it of that diversity, reducing it to a single formula of “success” then what other consequence could we possibly expect?

It is obvious that universality cannot serve everyone. Yes, life may be easier with wealth, but is the pursuit of that wealth truly enjoyable? And when the pursuit does not seem to draw nearer, when the destination shifts further away because the system itself is designed in such a manner, can such a chase still be called meaningful?

I am not suggesting that we should stop seeking to improve the quality of life. Rather, this improvement should arise only when such a pursuit truly aligns with oneself in that specific moment. Growth need not be a restless, continuous chase. It can unfold as the organic aftermath of stability.

If we understand that purpose belongs to who we are, rather than to what we are told, then the accumulation of wealth begins to pale in comparison to the exploration of the self. In that realization, we finally see what the pursuit is truly about.

And the most important truth we must internalize is this: we exist because existence itself is necessary. Our lives are not a cosmic gift, but our thinking is, and therefore, so is our individual purpose.



